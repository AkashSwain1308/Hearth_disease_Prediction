{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## 1. Dataset description<a id='data-desc'></a>\n\nThis dataset consists of 11 features and a target variable. It has 6 nominal variables and 5 numeric variables. The detailed description of all the features are as follows:\n\n**1. Age:** Patients Age in years (Numeric)<br>\n**2. Sex:** Gender of patient (Male - 1, Female - 0) (Nominal)<br>\n**3. Chest Pain Type:** Type of chest pain experienced by patient categorized into 1 typical, 2 typical angina, 3 non-        anginal pain, 4 asymptomatic (Nominal)<br>\n**4. resting bp s:** Level of blood pressure at resting mode in mm/HG (Numerical)<br>\n**5. cholestrol:** Serum cholestrol in mg/dl (Numeric)<br>\n**6. fasting blood sugar:** Blood sugar levels on fasting > 120 mg/dl represents as 1 in case of true and 0 as false (Nominal)<br>\n**7. resting ecg:** Result of electrocardiogram while at rest are represented in 3 distinct values 0 : Normal 1: Abnormality in ST-T wave 2: Left ventricular hypertrophy (Nominal)<br>\n**8. max heart rate:** Maximum heart rate achieved (Numeric)<br>\n**9. exercise angina:** Angina induced by exercise 0 depicting NO 1 depicting Yes (Nominal)<br>\n**10. oldpeak:** Exercise induced ST-depression in comparison with the state of rest (Numeric)<br>\n**11. ST slope:** ST segment measured in terms of slope during peak exercise 0: Normal 1: Upsloping 2: Flat 3: Downsloping (Nominal)<br>\n\n#### Target variable\n**12. target:** It is the target variable which we have to predict 1 means patient is suffering from heart risk and 0 means patient is normal.\n","metadata":{}},{"cell_type":"markdown","source":"## 2. Importing Libraries<a id='imp-lib'></a>","metadata":{}},{"cell_type":"code","source":"\nimport pandas as pd\nfrom pandas import DataFrame\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.metrics import log_loss,roc_auc_score,precision_score,f1_score,recall_score,roc_curve,auc\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix,accuracy_score,fbeta_score,matthews_corrcoef\nfrom sklearn import metrics\nfrom sklearn.metrics import log_loss\nfrom imblearn.metrics import geometric_mean_score\nimport warnings\nimport re\nimport sklearn\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nwarnings.filterwarnings('ignore')\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier,VotingClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.svm import SVC \nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom vecstack import stacking\nfrom scipy import stats\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-12-30T19:46:17.348138Z","iopub.execute_input":"2022-12-30T19:46:17.348529Z","iopub.status.idle":"2022-12-30T19:46:17.378697Z","shell.execute_reply.started":"2022-12-30T19:46:17.348493Z","shell.execute_reply":"2022-12-30T19:46:17.377941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Loading Dataset<a id='data-load'></a>","metadata":{}},{"cell_type":"code","source":"dt = pd.read_csv('../input/heart-disease-prediction/heart_disease_prediction.csv')","metadata":{"execution":{"iopub.status.busy":"2022-12-30T19:39:00.472532Z","iopub.execute_input":"2022-12-30T19:39:00.472914Z","iopub.status.idle":"2022-12-30T19:39:00.487222Z","shell.execute_reply.started":"2022-12-30T19:39:00.472869Z","shell.execute_reply":"2022-12-30T19:39:00.486234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets see some of the sample entries of dataset","metadata":{}},{"cell_type":"code","source":"dt.head()","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2022-12-30T19:39:00.488916Z","iopub.execute_input":"2022-12-30T19:39:00.489293Z","iopub.status.idle":"2022-12-30T19:39:00.507850Z","shell.execute_reply.started":"2022-12-30T19:39:00.489251Z","shell.execute_reply":"2022-12-30T19:39:00.506879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see from above dataset entries some of the features should be nominal and to be encoded as their category type. In the next step we will be encoding features to their respective category as per the dataset description.","metadata":{}},{"cell_type":"markdown","source":"## 4. Data Cleaning & Preprocessing <a id='data-prep'></a>\n In this step we will first change the name of columns as some of the columns have weird naming pattern and then we will encode the features into categorical variables","metadata":{}},{"cell_type":"code","source":"# renaming features to proper name\ndt.columns = ['age', 'sex', 'chest_pain_type', 'resting_blood_pressure', 'cholesterol', 'fasting_blood_sugar', 'rest_ecg', 'max_heart_rate_achieved',\n       'exercise_induced_angina', 'st_depression', 'st_slope','target']","metadata":{"execution":{"iopub.status.busy":"2022-12-30T19:39:00.509515Z","iopub.execute_input":"2022-12-30T19:39:00.509803Z","iopub.status.idle":"2022-12-30T19:39:00.518538Z","shell.execute_reply.started":"2022-12-30T19:39:00.509770Z","shell.execute_reply":"2022-12-30T19:39:00.517593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# converting features to categorical features \n\ndt['chest_pain_type'][dt['chest_pain_type'] == 1] = 'typical angina'\ndt['chest_pain_type'][dt['chest_pain_type'] == 2] = 'atypical angina'\ndt['chest_pain_type'][dt['chest_pain_type'] == 3] = 'non-anginal pain'\ndt['chest_pain_type'][dt['chest_pain_type'] == 4] = 'asymptomatic'\n\n\n\ndt['rest_ecg'][dt['rest_ecg'] == 0] = 'normal'\ndt['rest_ecg'][dt['rest_ecg'] == 1] = 'ST-T wave abnormality'\ndt['rest_ecg'][dt['rest_ecg'] == 2] = 'left ventricular hypertrophy'\n\n\ndt['st_slope'][dt['st_slope'] == 0] = 'normal'\ndt['st_slope'][dt['st_slope'] == 1] = 'upsloping'\ndt['st_slope'][dt['st_slope'] == 2] = 'flat'\ndt['st_slope'][dt['st_slope'] == 3] = 'downsloping'\n\ndt[\"sex\"] = dt.sex.apply(lambda  x:'male' if x==1 else 'female')","metadata":{"execution":{"iopub.status.busy":"2022-12-30T19:39:00.521209Z","iopub.execute_input":"2022-12-30T19:39:00.521567Z","iopub.status.idle":"2022-12-30T19:39:00.565490Z","shell.execute_reply.started":"2022-12-30T19:39:00.521472Z","shell.execute_reply":"2022-12-30T19:39:00.564664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checking the top 5 entries of dataset after feature encoding\ndt.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-30T19:39:00.566769Z","iopub.execute_input":"2022-12-30T19:39:00.567195Z","iopub.status.idle":"2022-12-30T19:39:00.584878Z","shell.execute_reply.started":"2022-12-30T19:39:00.567162Z","shell.execute_reply":"2022-12-30T19:39:00.583816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see features are encoded sucessfully to their respective categories. Next we will be checking if there is any missing entry or not ?","metadata":{}},{"cell_type":"code","source":"## Checking missing entries in the dataset columnwise\ndt.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2022-12-30T19:39:00.586362Z","iopub.execute_input":"2022-12-30T19:39:00.586664Z","iopub.status.idle":"2022-12-30T19:39:00.603446Z","shell.execute_reply.started":"2022-12-30T19:39:00.586634Z","shell.execute_reply":"2022-12-30T19:39:00.602353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So, there are no missing entries in the dataset thats great. Next we will move towards exploring the dataset by performing detailed EDA","metadata":{}},{"cell_type":"markdown","source":"**EXPLORATORY DATA ANALYSIS (EDA) and INITIAL DATA ANALYSIS (IDA)**","metadata":{}},{"cell_type":"code","source":"# first checking the shape of the dataset\ndt.shape","metadata":{"execution":{"iopub.status.busy":"2022-12-30T19:39:00.605066Z","iopub.execute_input":"2022-12-30T19:39:00.605611Z","iopub.status.idle":"2022-12-30T19:39:00.615028Z","shell.execute_reply.started":"2022-12-30T19:39:00.605568Z","shell.execute_reply":"2022-12-30T19:39:00.613647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So, there are total 1190 records and 11 features with 1 target variable. Lets check the summary of numerical and categorical features.","metadata":{}},{"cell_type":"code","source":"# summary statistics of numerical columns\ndt.describe(include =[np.number])","metadata":{"execution":{"iopub.status.busy":"2022-12-30T19:39:00.617764Z","iopub.execute_input":"2022-12-30T19:39:00.618087Z","iopub.status.idle":"2022-12-30T19:39:00.668377Z","shell.execute_reply.started":"2022-12-30T19:39:00.618046Z","shell.execute_reply":"2022-12-30T19:39:00.667401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see from above description resting_blood_pressure and cholestrol have some outliers as they have minimum value of 0 whereas cholestrol has outlier on upper side also having maximum value of 603.","metadata":{}},{"cell_type":"code","source":"# summary statistics of categorical columns\ndt.describe(include =[np.object])","metadata":{"execution":{"iopub.status.busy":"2022-12-30T19:39:00.670520Z","iopub.execute_input":"2022-12-30T19:39:00.670795Z","iopub.status.idle":"2022-12-30T19:39:00.702058Z","shell.execute_reply.started":"2022-12-30T19:39:00.670764Z","shell.execute_reply":"2022-12-30T19:39:00.700882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Distribution of Heart disease (target variable)","metadata":{}},{"cell_type":"code","source":"# Plotting attrition of employees\nfig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, sharey=False, figsize=(14,6))\n\nax1 = dt['target'].value_counts().plot.pie( x=\"Heart disease\" ,y ='no.of patients', \n                   autopct = \"%1.0f%%\",labels=[\"Heart Disease\",\"Normal\"], startangle = 180,ax=ax1);\nax1.set(title = 'Percentage of Heart disease patients in Dataset')\n\nax2 = dt[\"target\"].value_counts().plot(kind=\"barh\" ,ax =ax2)\nfor i,j in enumerate(dt[\"target\"].value_counts().values):\n    ax2.text(.5,i,j,fontsize=12)\nax2.set(title = 'No. of Heart disease patients in Dataset')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-12-30T19:39:00.703900Z","iopub.execute_input":"2022-12-30T19:39:00.704300Z","iopub.status.idle":"2022-12-30T19:39:00.921463Z","shell.execute_reply.started":"2022-12-30T19:39:00.704254Z","shell.execute_reply":"2022-12-30T19:39:00.920460Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The dataset is balanced having 629 heart disease patients and 561 normal patients","metadata":{}},{"cell_type":"markdown","source":"### Checking Gender & Agewise Distribution","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(18,12))\nplt.subplot(221)\ndt[\"sex\"].value_counts().plot.pie(autopct = \"%1.0f%%\",colors = sns.color_palette(),startangle = 90,labels=[\"Male\",\"Female\"],\nwedgeprops={\"linewidth\":2,\"edgecolor\":\"k\"},explode=[.1,.1],shadow =True)\nplt.title(\"Distribution of Gender\")\nplt.subplot(222)\nax= sns.distplot(dt['age'], rug=True)\nplt.title(\"Age wise distribution\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-12-30T19:39:24.819617Z","iopub.execute_input":"2022-12-30T19:39:24.819966Z","iopub.status.idle":"2022-12-30T19:39:25.124939Z","shell.execute_reply.started":"2022-12-30T19:39:24.819928Z","shell.execute_reply":"2022-12-30T19:39:25.123938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see from above plot, in this dataset males percentage is way too higher than females where as average age of patients is around 55.","metadata":{}},{"cell_type":"code","source":"attr_1=dt[dt['target']==1]\nattr_0=dt[dt['target']==0]\nfig = plt.figure(figsize=(15,5))\nax1 = plt.subplot2grid((1,2),(0,0))\nsns.distplot(attr_0['age'])\nplt.title('AGE DISTRIBUTION OF NORMAL PATIENTS', fontsize=15, weight='bold')\n\nax1 = plt.subplot2grid((1,2),(0,1))\nsns.countplot(attr_0['sex'], palette='viridis')\nplt.title('GENDER DISTRIBUTION OF NORMAL PATIENTS', fontsize=15, weight='bold' )\nplt.show()\n\nfig = plt.figure(figsize=(15,5))\nax1 = plt.subplot2grid((1,2),(0,0))\nsns.distplot(attr_1['age'])\nplt.title('AGE DISTRIBUTION OF HEART DISEASE PATIENTS', fontsize=15, weight='bold')\n\nax1 = plt.subplot2grid((1,2),(0,1))\nsns.countplot(attr_1['sex'], palette='viridis')\nplt.title('GENDER DISTRIBUTION OF HEART DISEASE PATIENTS', fontsize=15, weight='bold' )\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-12-30T19:39:33.574140Z","iopub.execute_input":"2022-12-30T19:39:33.574477Z","iopub.status.idle":"2022-12-30T19:39:34.220932Z","shell.execute_reply.started":"2022-12-30T19:39:33.574441Z","shell.execute_reply":"2022-12-30T19:39:34.220077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see from above plot more patients accounts for heart disease in comparison to females whereas mean age for heart disease patients is around 58 to 60 years","metadata":{}},{"cell_type":"markdown","source":"### Distribution of Chest Pain Type","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10,4))\n\n# Horizontal Bar Plot\ntitle_cnt=dt.chest_pain_type.value_counts().sort_values(ascending=False).reset_index()\nmn= ax.barh(title_cnt.iloc[:,0], title_cnt.iloc[:,1], color='silver')\nmn[0].set_color('lightskyblue')\nmn[3].set_color('crimson')\n\n\n# Remove axes splines\nfor s in ['top','bottom','left','right']:\n    ax.spines[s].set_visible(False)\n\n# Remove x,y Ticks\nax.xaxis.set_ticks_position('none')\nax.yaxis.set_ticks_position('none')\n\n# Add padding between axes and labels\nax.xaxis.set_tick_params(pad=5)\nax.yaxis.set_tick_params(pad=10)\n\n# Add x,y gridlines\nax.grid(b=True, color='grey', linestyle='-.', linewidth=1, alpha=0.4)\n\n# Show top values \nax.invert_yaxis()\n\n# Add Plot Title\nax.set_title('Chest Pain type Distribution',\n             loc='center', pad=10, fontsize=16)\nplt.yticks(weight='bold')\n\n\n# Add annotation to bars\nfor i in ax.patches:\n    ax.text(i.get_width()+10, i.get_y()+0.5, str(round((i.get_width()), 2)),\n            fontsize=10, fontweight='bold', color='grey')\nplt.yticks(weight='bold')\nplt.xticks(weight='bold')\n# Show Plot\nplt.show()\n\n\nfig, ax = plt.subplots(figsize=(10,4))\n\n# Horizontal Bar Plot\ntitle_cnt=attr_1.chest_pain_type.value_counts().sort_values(ascending=False).reset_index()\nmn= ax.barh(title_cnt.iloc[:,0], title_cnt.iloc[:,1], color='silver')\nmn[0].set_color('red')\nmn[3].set_color('blue')\n\n\n# Remove axes splines\nfor s in ['top','bottom','left','right']:\n    ax.spines[s].set_visible(False)\n\n# Remove x,y Ticks\nax.xaxis.set_ticks_position('none')\nax.yaxis.set_ticks_position('none')\n\n# Add padding between axes and labels\nax.xaxis.set_tick_params(pad=5)\nax.yaxis.set_tick_params(pad=10)\n\n# Add x,y gridlines\nax.grid(b=True, color='grey', linestyle='-.', linewidth=1, alpha=0.4)\n\n# Show top values \nax.invert_yaxis()\n\n# Add Plot Title\nax.set_title('Chest Pain type Distribution of Heart patients',\n             loc='center', pad=10, fontsize=16)\nplt.yticks(weight='bold')\n\n\n# Add annotation to bars\nfor i in ax.patches:\n    ax.text(i.get_width()+10, i.get_y()+0.5, str(round((i.get_width()), 2)),\n            fontsize=10, fontweight='bold', color='grey')\nplt.yticks(weight='bold')\nplt.xticks(weight='bold')\n# Show Plot\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-12-30T19:39:34.222782Z","iopub.execute_input":"2022-12-30T19:39:34.223027Z","iopub.status.idle":"2022-12-30T19:39:34.791046Z","shell.execute_reply.started":"2022-12-30T19:39:34.222999Z","shell.execute_reply":"2022-12-30T19:39:34.790159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Exploring the Heart Disease patients based on Chest Pain Type\nplot_criteria= ['chest_pain_type', 'target']\ncm = sns.light_palette(\"red\", as_cmap=True)\n(round(pd.crosstab(dt[plot_criteria[0]], dt[plot_criteria[1]], normalize='columns') * 100,2)).style.background_gradient(cmap = cm)","metadata":{"execution":{"iopub.status.busy":"2022-12-30T19:39:34.792223Z","iopub.execute_input":"2022-12-30T19:39:34.792474Z","iopub.status.idle":"2022-12-30T19:39:34.836400Z","shell.execute_reply.started":"2022-12-30T19:39:34.792446Z","shell.execute_reply":"2022-12-30T19:39:34.835469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see from above plot **76%** of the chest pain type of the heart disease patients have asymptomatic chest pain. \n\nAsymptomatic heart attacks medically known as **silent myocardial infarction (SMI)** annually accounts for around 45-50% of morbidities due to cardiac ailments and even premature deaths in India. The incidences among middle aged people experiencing SMI is twice likely to develop in males than females. The symptoms of SMI being very mild in comparison to an actual heart attack; it is described as a silent killer. Unlike the symptoms in a normal heart attack which includes extreme chest pain, stabbing pain in the arms, neck & jaw, sudden shortness of breath, sweating and dizziness, the symptoms of SMI are very brief and hence confused with regular discomfort and most often ignored.\n\n[reference](https://www.maxhealthcare.in/blogs/cardiology/rise-cases-asymptomatic-heart-attacks-amongst-middle-aged-people)","metadata":{}},{"cell_type":"markdown","source":"### Distribution of Rest ECG","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10,4))\n\n# Horizontal Bar Plot\ntitle_cnt=dt.rest_ecg.value_counts().sort_values(ascending=False).reset_index()\nmn= ax.barh(title_cnt.iloc[:,0], title_cnt.iloc[:,1], color='silver')\nmn[0].set_color('lightskyblue')\nmn[2].set_color('crimson')\n\n\n# Remove axes splines\nfor s in ['top','bottom','left','right']:\n    ax.spines[s].set_visible(False)\n\n# Remove x,y Ticks\nax.xaxis.set_ticks_position('none')\nax.yaxis.set_ticks_position('none')\n\n# Add padding between axes and labels\nax.xaxis.set_tick_params(pad=5)\nax.yaxis.set_tick_params(pad=10)\n\n# Add x,y gridlines\nax.grid(b=True, color='grey', linestyle='-.', linewidth=1, alpha=0.4)\n\n# Show top values \nax.invert_yaxis()\n\n# Add Plot Title\nax.set_title('Rest ECG Distribution',\n             loc='center', pad=10, fontsize=16)\nplt.yticks(weight='bold')\n\n\n# Add annotation to bars\nfor i in ax.patches:\n    ax.text(i.get_width()+10, i.get_y()+0.5, str(round((i.get_width()), 2)),\n            fontsize=10, fontweight='bold', color='grey')\nplt.yticks(weight='bold')\nplt.xticks(weight='bold')\n# Show Plot\nplt.show()\n\n\nfig, ax = plt.subplots(figsize=(10,4))\n\n# Horizontal Bar Plot\ntitle_cnt=attr_1.rest_ecg.value_counts().sort_values(ascending=False).reset_index()\nmn= ax.barh(title_cnt.iloc[:,0], title_cnt.iloc[:,1], color='silver')\nmn[0].set_color('red')\nmn[2].set_color('blue')\n\n\n# Remove axes splines\nfor s in ['top','bottom','left','right']:\n    ax.spines[s].set_visible(False)\n\n# Remove x,y Ticks\nax.xaxis.set_ticks_position('none')\nax.yaxis.set_ticks_position('none')\n\n# Add padding between axes and labels\nax.xaxis.set_tick_params(pad=5)\nax.yaxis.set_tick_params(pad=10)\n\n# Add x,y gridlines\nax.grid(b=True, color='grey', linestyle='-.', linewidth=1, alpha=0.4)\n\n# Show top values \nax.invert_yaxis()\n\n# Add Plot Title\nax.set_title('Rest ECG Distribution of Heart patients',\n             loc='center', pad=10, fontsize=16)\nplt.yticks(weight='bold')\n\n\n# Add annotation to bars\nfor i in ax.patches:\n    ax.text(i.get_width()+10, i.get_y()+0.5, str(round((i.get_width()), 2)),\n            fontsize=10, fontweight='bold', color='grey')\nplt.yticks(weight='bold')\nplt.xticks(weight='bold')\n# Show Plot\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-12-30T19:39:34.838357Z","iopub.execute_input":"2022-12-30T19:39:34.838716Z","iopub.status.idle":"2022-12-30T19:39:35.414530Z","shell.execute_reply.started":"2022-12-30T19:39:34.838674Z","shell.execute_reply":"2022-12-30T19:39:35.413350Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Exploring the Heart Disease patients based on REST ECG\nplot_criteria= ['rest_ecg', 'target']\ncm = sns.light_palette(\"red\", as_cmap=True)\n(round(pd.crosstab(dt[plot_criteria[0]], dt[plot_criteria[1]], normalize='columns') * 100,2)).style.background_gradient(cmap = cm)","metadata":{"execution":{"iopub.status.busy":"2022-12-30T19:39:35.418795Z","iopub.execute_input":"2022-12-30T19:39:35.419086Z","iopub.status.idle":"2022-12-30T19:39:35.464496Z","shell.execute_reply.started":"2022-12-30T19:39:35.419056Z","shell.execute_reply":"2022-12-30T19:39:35.463528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10,4))\n\n# Horizontal Bar Plot\ntitle_cnt=dt.st_slope.value_counts().sort_values(ascending=False).reset_index()\nmn= ax.barh(title_cnt.iloc[:,0], title_cnt.iloc[:,1], color='silver')\nmn[0].set_color('lightskyblue')\nmn[3].set_color('crimson')\n\n\n# Remove axes splines\nfor s in ['top','bottom','left','right']:\n    ax.spines[s].set_visible(False)\n\n# Remove x,y Ticks\nax.xaxis.set_ticks_position('none')\nax.yaxis.set_ticks_position('none')\n\n# Add padding between axes and labels\nax.xaxis.set_tick_params(pad=5)\nax.yaxis.set_tick_params(pad=10)\n\n# Add x,y gridlines\nax.grid(b=True, color='grey', linestyle='-.', linewidth=1, alpha=0.4)\n\n# Show top values \nax.invert_yaxis()\n\n# Add Plot Title\nax.set_title('ST Slope Distribution',\n             loc='center', pad=10, fontsize=16)\nplt.yticks(weight='bold')\n\n\n# Add annotation to bars\nfor i in ax.patches:\n    ax.text(i.get_width()+10, i.get_y()+0.5, str(round((i.get_width()), 2)),\n            fontsize=10, fontweight='bold', color='grey')\nplt.yticks(weight='bold')\nplt.xticks(weight='bold')\n# Show Plot\nplt.show()\n\n\nfig, ax = plt.subplots(figsize=(10,4))\n\n# Horizontal Bar Plot\ntitle_cnt=attr_1.st_slope.value_counts().sort_values(ascending=False).reset_index()\nmn= ax.barh(title_cnt.iloc[:,0], title_cnt.iloc[:,1], color='silver')\nmn[0].set_color('red')\nmn[3].set_color('blue')\n\n\n# Remove axes splines\nfor s in ['top','bottom','left','right']:\n    ax.spines[s].set_visible(False)\n\n# Remove x,y Ticks\nax.xaxis.set_ticks_position('none')\nax.yaxis.set_ticks_position('none')\n\n# Add padding between axes and labels\nax.xaxis.set_tick_params(pad=5)\nax.yaxis.set_tick_params(pad=10)\n\n# Add x,y gridlines\nax.grid(b=True, color='grey', linestyle='-.', linewidth=1, alpha=0.4)\n\n# Show top values \nax.invert_yaxis()\n\n# Add Plot Title\nax.set_title('ST Slope Distribution of Heart patients',\n             loc='center', pad=10, fontsize=16)\nplt.yticks(weight='bold')\n\n\n# Add annotation to bars\nfor i in ax.patches:\n    ax.text(i.get_width()+10, i.get_y()+0.5, str(round((i.get_width()), 2)),\n            fontsize=10, fontweight='bold', color='grey')\nplt.yticks(weight='bold')\nplt.xticks(weight='bold')\n# Show Plot\nplt.show()","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-12-30T19:39:35.468152Z","iopub.execute_input":"2022-12-30T19:39:35.468436Z","iopub.status.idle":"2022-12-30T19:39:36.029008Z","shell.execute_reply.started":"2022-12-30T19:39:35.468406Z","shell.execute_reply":"2022-12-30T19:39:36.028115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Exploring the Heart Disease patients based on ST Slope\nplot_criteria= ['st_slope', 'target']\ncm = sns.light_palette(\"red\", as_cmap=True)\n(round(pd.crosstab(dt[plot_criteria[0]], dt[plot_criteria[1]], normalize='columns') * 100,2)).style.background_gradient(cmap = cm)","metadata":{"execution":{"iopub.status.busy":"2022-12-30T19:39:36.030446Z","iopub.execute_input":"2022-12-30T19:39:36.030698Z","iopub.status.idle":"2022-12-30T19:39:36.073283Z","shell.execute_reply.started":"2022-12-30T19:39:36.030669Z","shell.execute_reply":"2022-12-30T19:39:36.072451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Distribution of Numerical features","metadata":{}},{"cell_type":"code","source":"sns.pairplot(dt, hue = 'target', vars = ['age', 'resting_blood_pressure', 'cholesterol'] )","metadata":{"execution":{"iopub.status.busy":"2022-12-30T19:39:36.074663Z","iopub.execute_input":"2022-12-30T19:39:36.074914Z","iopub.status.idle":"2022-12-30T19:39:39.954712Z","shell.execute_reply.started":"2022-12-30T19:39:36.074886Z","shell.execute_reply":"2022-12-30T19:39:39.953496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above plot it is clear that as the age increases chances of heart disease increases","metadata":{}},{"cell_type":"code","source":"sns.scatterplot(x = 'resting_blood_pressure', y = 'cholesterol', hue = 'target', data = dt)","metadata":{"execution":{"iopub.status.busy":"2022-12-30T19:39:39.956020Z","iopub.execute_input":"2022-12-30T19:39:39.956298Z","iopub.status.idle":"2022-12-30T19:39:40.281937Z","shell.execute_reply.started":"2022-12-30T19:39:39.956268Z","shell.execute_reply":"2022-12-30T19:39:40.280913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above plot we can see outliers clearly as for some of the patients cholestrol is 0 whereas for one patient both cholestrol and resting bp is 0 which is may be due to missing entries we will filter these ouliers later","metadata":{}},{"cell_type":"code","source":"sns.scatterplot(x = 'resting_blood_pressure', y = 'age', hue = 'target', data = dt)","metadata":{"execution":{"iopub.status.busy":"2022-12-30T19:39:40.283589Z","iopub.execute_input":"2022-12-30T19:39:40.283949Z","iopub.status.idle":"2022-12-30T19:39:40.607521Z","shell.execute_reply.started":"2022-12-30T19:39:40.283904Z","shell.execute_reply":"2022-12-30T19:39:40.606770Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6. Outlier Detection & Removal <a id='data-out'></a>","metadata":{}},{"cell_type":"markdown","source":"Outliers are data point which are too different from rest of the data","metadata":{}},{"cell_type":"code","source":"# filtering numeric features as age , resting bp, cholestrol and max heart rate achieved has outliers as per EDA\n\ndt_numeric = dt[['age','resting_blood_pressure','cholesterol','max_heart_rate_achieved']]","metadata":{"execution":{"iopub.status.busy":"2022-12-30T19:39:40.608933Z","iopub.execute_input":"2022-12-30T19:39:40.609202Z","iopub.status.idle":"2022-12-30T19:39:40.614926Z","shell.execute_reply.started":"2022-12-30T19:39:40.609172Z","shell.execute_reply":"2022-12-30T19:39:40.614164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dt_numeric.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-30T19:39:40.616197Z","iopub.execute_input":"2022-12-30T19:39:40.616674Z","iopub.status.idle":"2022-12-30T19:39:40.633699Z","shell.execute_reply.started":"2022-12-30T19:39:40.616633Z","shell.execute_reply":"2022-12-30T19:39:40.632628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# calculating zscore of numeric columns in the dataset\nz = np.abs(stats.zscore(dt_numeric))\nprint(z)","metadata":{"execution":{"iopub.status.busy":"2022-12-30T19:39:40.635836Z","iopub.execute_input":"2022-12-30T19:39:40.636436Z","iopub.status.idle":"2022-12-30T19:39:40.646631Z","shell.execute_reply.started":"2022-12-30T19:39:40.636400Z","shell.execute_reply":"2022-12-30T19:39:40.645489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"from these points it is diffciult to say which points are outliers so we will now define threshold","metadata":{}},{"cell_type":"code","source":"# Defining threshold for filtering outliers \nthreshold = 3\nprint(np.where(z > 3))","metadata":{"execution":{"iopub.status.busy":"2022-12-30T19:39:40.648197Z","iopub.execute_input":"2022-12-30T19:39:40.648519Z","iopub.status.idle":"2022-12-30T19:39:40.659739Z","shell.execute_reply.started":"2022-12-30T19:39:40.648480Z","shell.execute_reply":"2022-12-30T19:39:40.658907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Don’t be confused by the results. The first array contains the list of row numbers and second array respective column numbers, which mean z[30][2] have a Z-score higher than 3. There are total 17 data points which are outliers.","metadata":{}},{"cell_type":"code","source":"#filtering outliers retaining only those data points which are below threshhold\ndt = dt[(z < 3).all(axis=1)]","metadata":{"execution":{"iopub.status.busy":"2022-12-30T19:39:40.661350Z","iopub.execute_input":"2022-12-30T19:39:40.661618Z","iopub.status.idle":"2022-12-30T19:39:40.674728Z","shell.execute_reply.started":"2022-12-30T19:39:40.661587Z","shell.execute_reply":"2022-12-30T19:39:40.673752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checking shape of dataset after outlier removal\ndt.shape","metadata":{"execution":{"iopub.status.busy":"2022-12-30T19:39:40.676210Z","iopub.execute_input":"2022-12-30T19:39:40.676483Z","iopub.status.idle":"2022-12-30T19:39:40.686873Z","shell.execute_reply.started":"2022-12-30T19:39:40.676447Z","shell.execute_reply":"2022-12-30T19:39:40.686106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Great !! all the 17 data points which are outliers are now removed.\n\nNow before splitting dataset into train and test we first encode categorical variables as dummy variables and segregate feature and target variable.","metadata":{}},{"cell_type":"code","source":"## encoding categorical variables\ndt = pd.get_dummies(dt, drop_first=True)\n\ndt.head()\n","metadata":{"execution":{"iopub.status.busy":"2022-12-30T19:39:40.688275Z","iopub.execute_input":"2022-12-30T19:39:40.688684Z","iopub.status.idle":"2022-12-30T19:39:40.726367Z","shell.execute_reply.started":"2022-12-30T19:39:40.688652Z","shell.execute_reply":"2022-12-30T19:39:40.725413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checking the shape of dataset\ndt.shape","metadata":{"execution":{"iopub.status.busy":"2022-12-30T19:39:40.727798Z","iopub.execute_input":"2022-12-30T19:39:40.728066Z","iopub.status.idle":"2022-12-30T19:39:40.733825Z","shell.execute_reply.started":"2022-12-30T19:39:40.728022Z","shell.execute_reply":"2022-12-30T19:39:40.732943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# segregating dataset into features i.e., X and target variables i.e., y\nX = dt.drop(['target'],axis=1)\ny = dt['target']","metadata":{"execution":{"iopub.status.busy":"2022-12-30T19:39:40.735071Z","iopub.execute_input":"2022-12-30T19:39:40.735350Z","iopub.status.idle":"2022-12-30T19:39:40.745496Z","shell.execute_reply.started":"2022-12-30T19:39:40.735321Z","shell.execute_reply":"2022-12-30T19:39:40.744707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 7. Train Test Split <a id='data-train'></a>","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2,shuffle=True, random_state=5)","metadata":{"execution":{"iopub.status.busy":"2022-12-30T19:39:40.746571Z","iopub.execute_input":"2022-12-30T19:39:40.746942Z","iopub.status.idle":"2022-12-30T19:39:40.763315Z","shell.execute_reply.started":"2022-12-30T19:39:40.746912Z","shell.execute_reply":"2022-12-30T19:39:40.762180Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('------------Training Set------------------')\nprint(X_train.shape)\nprint(y_train.shape)\n\nprint('------------Test Set------------------')\nprint(X_test.shape)\nprint(y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-12-30T19:39:40.765622Z","iopub.execute_input":"2022-12-30T19:39:40.766325Z","iopub.status.idle":"2022-12-30T19:39:40.775248Z","shell.execute_reply.started":"2022-12-30T19:39:40.766280Z","shell.execute_reply":"2022-12-30T19:39:40.774018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### feature normalization\nIn this step we will normalize all the numeric feature in the range of 0 to 1","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nX_train[['age','resting_blood_pressure','cholesterol','max_heart_rate_achieved','st_depression']] = scaler.fit_transform(X_train[['age','resting_blood_pressure','cholesterol','max_heart_rate_achieved','st_depression']])\nX_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-30T19:39:40.776589Z","iopub.execute_input":"2022-12-30T19:39:40.777057Z","iopub.status.idle":"2022-12-30T19:39:40.807740Z","shell.execute_reply.started":"2022-12-30T19:39:40.776999Z","shell.execute_reply":"2022-12-30T19:39:40.806609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test[['age','resting_blood_pressure','cholesterol','max_heart_rate_achieved','st_depression']] = scaler.transform(X_test[['age','resting_blood_pressure','cholesterol','max_heart_rate_achieved','st_depression']])\nX_test.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-30T19:39:40.809310Z","iopub.execute_input":"2022-12-30T19:39:40.809585Z","iopub.status.idle":"2022-12-30T19:39:40.840154Z","shell.execute_reply.started":"2022-12-30T19:39:40.809552Z","shell.execute_reply":"2022-12-30T19:39:40.838930Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 8. Cross Validation <a id='cross-val'></a>\n\nIn this step, we will build different baseline models and perform 10-fold cross validation","metadata":{}},{"cell_type":"code","source":"from sklearn import model_selection\nfrom sklearn.model_selection import cross_val_score\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\n# function initializing baseline machine learning models\ndef GetBasedModel():\n    basedModels = []\n    basedModels.append(('LR_L2'   , LogisticRegression(penalty='l2')))\n    basedModels.append(('LDA'  , LinearDiscriminantAnalysis()))\n    basedModels.append(('KNN7'  , KNeighborsClassifier(7)))\n    basedModels.append(('KNN5'  , KNeighborsClassifier(5)))\n    basedModels.append(('KNN9'  , KNeighborsClassifier(9)))\n    basedModels.append(('KNN11'  , KNeighborsClassifier(11)))\n    basedModels.append(('CART' , DecisionTreeClassifier()))\n    basedModels.append(('NB'   , GaussianNB()))\n    basedModels.append(('SVM Linear'  , SVC(kernel='linear',gamma='auto',probability=True)))\n    basedModels.append(('SVM RBF'  , SVC(kernel='rbf',gamma='auto',probability=True)))\n    basedModels.append(('AB'   , AdaBoostClassifier()))\n    basedModels.append(('GBM'  , GradientBoostingClassifier(n_estimators=100,max_features='sqrt')))\n    basedModels.append(('RF_Ent100'   , RandomForestClassifier(criterion='entropy',n_estimators=100)))\n    basedModels.append(('RF_Gini100'   , RandomForestClassifier(criterion='gini',n_estimators=100)))\n    basedModels.append(('ET100'   , ExtraTreesClassifier(n_estimators= 100)))\n    basedModels.append(('ET500'   , ExtraTreesClassifier(n_estimators= 500)))\n    basedModels.append(('MLP', MLPClassifier()))\n    basedModels.append(('SGD3000', SGDClassifier(max_iter=1000, tol=1e-4)))\n    basedModels.append(('XGB_2000', xgb.XGBClassifier(n_estimators= 2000)))\n    basedModels.append(('XGB_500', xgb.XGBClassifier(n_estimators= 500)))\n    basedModels.append(('XGB_100', xgb.XGBClassifier(n_estimators= 100)))\n    basedModels.append(('XGB_1000', xgb.XGBClassifier(n_estimators= 1000)))\n    basedModels.append(('ET1000'   , ExtraTreesClassifier(n_estimators= 1000)))\n    \n    return basedModels\n\n# function for performing 10-fold cross validation of all the baseline models\ndef BasedLine2(X_train, y_train,models):\n    # Test options and evaluation metric\n    num_folds = 10\n    scoring = 'accuracy'\n    seed = 7\n    results = []\n    names = []\n    for name, model in models:\n        kfold = model_selection.KFold(n_splits=10, random_state=seed)\n        cv_results = model_selection.cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n        results.append(cv_results)\n        names.append(name)\n        msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n        print(msg)\n         \n        \n    return results,msg","metadata":{"execution":{"iopub.status.busy":"2022-12-30T19:43:26.250508Z","iopub.execute_input":"2022-12-30T19:43:26.250847Z","iopub.status.idle":"2022-12-30T19:43:26.272667Z","shell.execute_reply.started":"2022-12-30T19:43:26.250811Z","shell.execute_reply":"2022-12-30T19:43:26.271176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models = GetBasedModel()\nnames,results = BasedLine2(X_train, y_train,models)","metadata":{"execution":{"iopub.status.busy":"2022-12-30T19:43:26.274900Z","iopub.execute_input":"2022-12-30T19:43:26.275277Z","iopub.status.idle":"2022-12-30T19:44:40.022460Z","shell.execute_reply.started":"2022-12-30T19:43:26.275231Z","shell.execute_reply":"2022-12-30T19:44:40.021529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 9. Model building <a id='data-model'></a>","metadata":{}},{"cell_type":"markdown","source":"### Random Forest Classifier (criterion = 'entropy')","metadata":{}},{"cell_type":"code","source":"rf_ent = RandomForestClassifier(criterion='entropy',n_estimators=100)\nrf_ent.fit(X_train, y_train)\ny_pred_rfe = rf_ent.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-12-30T19:44:40.025284Z","iopub.execute_input":"2022-12-30T19:44:40.025655Z","iopub.status.idle":"2022-12-30T19:44:40.284077Z","shell.execute_reply.started":"2022-12-30T19:44:40.025609Z","shell.execute_reply":"2022-12-30T19:44:40.283074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Multi Layer Perceptron","metadata":{}},{"cell_type":"code","source":"mlp = MLPClassifier()\nmlp.fit(X_train,y_train)\ny_pred_mlp = mlp.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-12-30T19:44:40.285612Z","iopub.execute_input":"2022-12-30T19:44:40.285899Z","iopub.status.idle":"2022-12-30T19:44:42.424216Z","shell.execute_reply.started":"2022-12-30T19:44:40.285870Z","shell.execute_reply":"2022-12-30T19:44:42.422817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### K nearest neighbour (n=9)","metadata":{}},{"cell_type":"code","source":"knn = KNeighborsClassifier(9)\nknn.fit(X_train,y_train)\ny_pred_knn = knn.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-12-30T19:44:42.428090Z","iopub.execute_input":"2022-12-30T19:44:42.428819Z","iopub.status.idle":"2022-12-30T19:44:42.481021Z","shell.execute_reply.started":"2022-12-30T19:44:42.428770Z","shell.execute_reply":"2022-12-30T19:44:42.479618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Extra Tree Classifier (n_estimators=500)","metadata":{}},{"cell_type":"code","source":"et_500 = ExtraTreesClassifier(n_estimators= 500)\net_500.fit(X_train,y_train)\ny_pred_et500 = et_500.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-12-30T19:44:42.484855Z","iopub.execute_input":"2022-12-30T19:44:42.485598Z","iopub.status.idle":"2022-12-30T19:44:43.673011Z","shell.execute_reply.started":"2022-12-30T19:44:42.485548Z","shell.execute_reply":"2022-12-30T19:44:43.672130Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### XGBoost (n_estimators=100)","metadata":{}},{"cell_type":"code","source":"xgb = xgb.XGBClassifier(n_estimators= 100)\nxgb.fit(X_train,y_train)\ny_pred_xgb = xgb.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-12-30T19:44:43.674623Z","iopub.execute_input":"2022-12-30T19:44:43.674978Z","iopub.status.idle":"2022-12-30T19:44:43.768771Z","shell.execute_reply.started":"2022-12-30T19:44:43.674935Z","shell.execute_reply":"2022-12-30T19:44:43.767957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Support Vector Classifier (kernel='linear')","metadata":{}},{"cell_type":"code","source":"svc = SVC(kernel='linear',gamma='auto',probability=True)\nsvc.fit(X_train,y_train)\ny_pred_svc = svc.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-12-30T19:44:43.770333Z","iopub.execute_input":"2022-12-30T19:44:43.770939Z","iopub.status.idle":"2022-12-30T19:44:43.848920Z","shell.execute_reply.started":"2022-12-30T19:44:43.770897Z","shell.execute_reply":"2022-12-30T19:44:43.848097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Stochastic Gradient Descent","metadata":{}},{"cell_type":"code","source":"sgd = SGDClassifier(max_iter=1000, tol=1e-4)\nsgd.fit(X_train,y_train)\ny_pred_sgd = sgd.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-12-30T19:44:43.850164Z","iopub.execute_input":"2022-12-30T19:44:43.850425Z","iopub.status.idle":"2022-12-30T19:44:43.864226Z","shell.execute_reply.started":"2022-12-30T19:44:43.850396Z","shell.execute_reply":"2022-12-30T19:44:43.862906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Adaboost Classifier","metadata":{}},{"cell_type":"code","source":"ada = AdaBoostClassifier()\nada.fit(X_train,y_train)\ny_pred_ada = ada.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-12-30T19:44:43.865571Z","iopub.execute_input":"2022-12-30T19:44:43.865997Z","iopub.status.idle":"2022-12-30T19:44:43.998264Z","shell.execute_reply.started":"2022-12-30T19:44:43.865961Z","shell.execute_reply":"2022-12-30T19:44:43.997318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### decision Tree Classifier (CART)","metadata":{}},{"cell_type":"code","source":"decc = DecisionTreeClassifier()\ndecc.fit(X_train,y_train)\ny_pred_decc = decc.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-12-30T19:44:43.999785Z","iopub.execute_input":"2022-12-30T19:44:44.000073Z","iopub.status.idle":"2022-12-30T19:44:44.011632Z","shell.execute_reply.started":"2022-12-30T19:44:44.000018Z","shell.execute_reply":"2022-12-30T19:44:44.010798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### gradient boosting machine ","metadata":{}},{"cell_type":"code","source":"gbm = GradientBoostingClassifier(n_estimators=100,max_features='sqrt')\ngbm.fit(X_train,y_train)\ny_pred_gbm = gbm.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-12-30T19:44:44.012897Z","iopub.execute_input":"2022-12-30T19:44:44.013506Z","iopub.status.idle":"2022-12-30T19:44:44.116736Z","shell.execute_reply.started":"2022-12-30T19:44:44.013468Z","shell.execute_reply":"2022-12-30T19:44:44.115782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 10. Model Evaluation  <a id='model-eval'></a>\n\n In this step we will first define which evaluation metrics we will use to evaluate our model. The most important evaluation metric for this problem domain is **sensitivity, specificity, Precision, F1-measure, Geometric mean and mathew correlation coefficient and finally ROC AUC curve**\n \n### Sensitivity vs Specificity","metadata":{}},{"cell_type":"markdown","source":"![](https://i.ibb.co/d43FVfJ/Sensitivity-and-specificity-svg.png)\n\n### Mathew Correlation coefficient (MCC)\n\nThe Matthews correlation coefficient (MCC), instead, is a more reliable statistical rate which produces a high score only if the prediction obtained good results in all of the four confusion matrix categories (true positives, false negatives, true negatives, and false positives), proportionally both to the size of positive elements and the size of negative elements in the dataset.\n\n![](https://i.ibb.co/mH6MmG4/mcc.jpg)","metadata":{}},{"cell_type":"markdown","source":"### Log Loss\nLogarithmic loss  measures the performance of a classification model where the prediction input is a probability value between 0 and 1. The goal of our machine learning models is to minimize this value. A perfect model would have a log loss of 0. Log loss increases as the predicted probability diverges from the actual label. So predicting a probability of .012 when the actual observation label is 1 would be bad and result in a high log loss.\n\nThe graph below shows the range of possible log loss values given a true observation (isDog = 1). As the predicted probability approaches 1, log loss slowly decreases. As the predicted probability decreases, however, the log loss increases rapidly. Log loss penalizes both types of errors, but especially those predications that are confident and wrong!\n\n![](https://i.ibb.co/6BdDczW/log-loss.jpg)","metadata":{}},{"cell_type":"markdown","source":"### F1 Score\n\n F1 Score is the weighted average of Precision and Recall. Therefore, this score takes both false positives and false negatives into account. Intuitively it is not as easy to understand as accuracy, but F1 is usually more useful than accuracy, especially if you have an uneven class distribution. Accuracy works best if false positives and false negatives have similar cost. If the cost of false positives and false negatives are very different, it’s better to look at both Precision and Recall. In our case, F1 score is 0.701.\n\n**F1 Score = 2*(Recall * Precision) / (Recall + Precision)**","metadata":{}},{"cell_type":"code","source":"CM=confusion_matrix(y_test,y_pred)\nsns.heatmap(CM, annot=True)\n\nTN = CM[0][0]\nFN = CM[1][0]\nTP = CM[1][1]\nFP = CM[0][1]\nspecificity = TN/(TN+FP)\nloss_log = log_loss(y_test, y_pred)\nacc= accuracy_score(y_test, y_pred)\nroc=roc_auc_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nmathew = matthews_corrcoef(y_test, y_pred)\nmodel_results =pd.DataFrame([['STacked Classifier',acc, prec,rec,specificity, f1,roc, loss_log,mathew]],\n               columns = ['Model', 'Accuracy','Precision', 'Sensitivity','Specificity', 'F1 Score','ROC','Log_Loss','mathew_corrcoef'])\n\nmodel_results","metadata":{"execution":{"iopub.status.busy":"2022-12-30T19:44:44.118383Z","iopub.execute_input":"2022-12-30T19:44:44.118656Z","iopub.status.idle":"2022-12-30T19:44:44.387657Z","shell.execute_reply.started":"2022-12-30T19:44:44.118626Z","shell.execute_reply":"2022-12-30T19:44:44.386747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Comparison with other Models","metadata":{}},{"cell_type":"code","source":"data = {'Random Forest': y_pred_rfe, \n                'MLP': y_pred_mlp, \n                'KNN': y_pred_knn, \n                'EXtra tree classifier': y_pred_et500,\n                'XGB': y_pred_xgb, \n                'SVC': y_pred_svc, \n                'SGD': y_pred_sgd,\n                'Adaboost': y_pred_ada, \n                'CART': y_pred_decc, \n                'GBM': y_pred_gbm }\n\nmodels = pd.DataFrame(data) \n \nfor column in models:\n    CM=confusion_matrix(y_test,models[column])\n    \n    TN = CM[0][0]\n    FN = CM[1][0]\n    TP = CM[1][1]\n    FP = CM[0][1]\n    specificity = TN/(TN+FP)\n    loss_log = log_loss(y_test, models[column])\n    acc= accuracy_score(y_test, models[column])\n    roc=roc_auc_score(y_test, models[column])\n    prec = precision_score(y_test, models[column])\n    rec = recall_score(y_test, models[column])\n    f1 = f1_score(y_test, models[column])\n    \n    mathew = matthews_corrcoef(y_test, models[column])\n    results =pd.DataFrame([[column,acc, prec,rec,specificity, f1,roc, loss_log,mathew]],\n               columns = ['Model', 'Accuracy','Precision', 'Sensitivity','Specificity', 'F1 Score','ROC','Log_Loss','mathew_corrcoef'])\n    model_results = model_results.append(results, ignore_index = True)\n\nmodel_results\n","metadata":{"execution":{"iopub.status.busy":"2022-12-30T19:44:44.389017Z","iopub.execute_input":"2022-12-30T19:44:44.389282Z","iopub.status.idle":"2022-12-30T19:44:44.546144Z","shell.execute_reply.started":"2022-12-30T19:44:44.389254Z","shell.execute_reply":"2022-12-30T19:44:44.545065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Findings\n- AS we can see from above results, Stacked Ensemble Classifier is best performer as it has highest test accuracy of 0.906, sensitivity of 0.943 and specificity of 0.866 and highest f1-score of 0.9133 and lowest Log Loss of 3.23 and highest ROC value of 0.904\n- Random Forest & Extra Tree classifier are second best having same performance measure in every aspect\n- XGboost has third best sensitivity level","metadata":{}},{"cell_type":"markdown","source":"### ROC AUC Curve","metadata":{}},{"cell_type":"code","source":"def roc_auc_plot(y_true, y_proba, label=' ', l='-', lw=1.0):\n    from sklearn.metrics import roc_curve, roc_auc_score\n    fpr, tpr, _ = roc_curve(y_true, y_proba[:,1])\n    ax.plot(fpr, tpr, linestyle=l, linewidth=lw,\n            label=\"%s (area=%.3f)\"%(label,roc_auc_score(y_true, y_proba[:,1])))\n\nf, ax = plt.subplots(figsize=(12,8))\n\nroc_auc_plot(y_test,model.predict_proba(S_test),label='Stacked Classifier ',l='-')\nroc_auc_plot(y_test,rf_ent.predict_proba(X_test),label='Random Forest Classifier ',l='-')\nroc_auc_plot(y_test,et_500.predict_proba(X_test),label='Extra Tree Classifier ',l='-')\nroc_auc_plot(y_test,xgb.predict_proba(X_test),label='XGboost',l='-')\n\nax.plot([0,1], [0,1], color='k', linewidth=0.5, linestyle='--', \n        )    \nax.legend(loc=\"lower right\")    \nax.set_xlabel('False Positive Rate')\nax.set_ylabel('True Positive Rate')\nax.set_xlim([0, 1])\nax.set_ylim([0, 1])\nax.set_title('Receiver Operator Characteristic curves')\nsns.despine()","metadata":{"execution":{"iopub.status.busy":"2022-12-30T19:44:44.547792Z","iopub.execute_input":"2022-12-30T19:44:44.548156Z","iopub.status.idle":"2022-12-30T19:44:44.927084Z","shell.execute_reply.started":"2022-12-30T19:44:44.548112Z","shell.execute_reply":"2022-12-30T19:44:44.926152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see highest average area under the curve (AUC) of 0.950 is attained by Extra Tree Classifier","metadata":{}},{"cell_type":"markdown","source":"As we have seen Randomforest is second best performing model in every performance measure. So we will be plotting SHAP feature importance plot using random forest to know which features are actually contributing in model's prediction.","metadata":{}},{"cell_type":"code","source":"num_feats=11\n\ndef cor_selector(X, y,num_feats):\n    cor_list = []\n    feature_name = X.columns.tolist()\n    # calculate the correlation with y for each feature\n    for i in X.columns.tolist():\n        cor = np.corrcoef(X[i], y)[0, 1]\n        cor_list.append(cor)\n    # replace NaN with 0\n    cor_list = [0 if np.isnan(i) else i for i in cor_list]\n    # feature name\n    cor_feature = X.iloc[:,np.argsort(np.abs(cor_list))[-num_feats:]].columns.tolist()\n    # feature selection? 0 for not select, 1 for select\n    cor_support = [True if i in cor_feature else False for i in feature_name]\n    return cor_support, cor_feature\ncor_support, cor_feature = cor_selector(X, y,num_feats)\nprint(str(len(cor_feature)), 'selected features')","metadata":{"execution":{"iopub.status.busy":"2022-12-30T19:44:44.928456Z","iopub.execute_input":"2022-12-30T19:44:44.928744Z","iopub.status.idle":"2022-12-30T19:44:44.948320Z","shell.execute_reply.started":"2022-12-30T19:44:44.928711Z","shell.execute_reply":"2022-12-30T19:44:44.947358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn.preprocessing import MinMaxScaler\nX_norm = MinMaxScaler().fit_transform(X)\nchi_selector = SelectKBest(chi2, k=num_feats)\nchi_selector.fit(X_norm, y)\nchi_support = chi_selector.get_support()\nchi_feature = X.loc[:,chi_support].columns.tolist()\nprint(str(len(chi_feature)), 'selected features')","metadata":{"execution":{"iopub.status.busy":"2022-12-30T19:44:44.949754Z","iopub.execute_input":"2022-12-30T19:44:44.950021Z","iopub.status.idle":"2022-12-30T19:44:44.963743Z","shell.execute_reply.started":"2022-12-30T19:44:44.949992Z","shell.execute_reply":"2022-12-30T19:44:44.962881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\nrfe_selector = RFE(estimator=LogisticRegression(), n_features_to_select=num_feats, step=10, verbose=5)\nrfe_selector.fit(X_norm, y)\nrfe_support = rfe_selector.get_support()\nrfe_feature = X.loc[:,rfe_support].columns.tolist()\nprint(str(len(rfe_feature)), 'selected features')","metadata":{"execution":{"iopub.status.busy":"2022-12-30T19:44:44.965500Z","iopub.execute_input":"2022-12-30T19:44:44.965789Z","iopub.status.idle":"2022-12-30T19:44:45.063149Z","shell.execute_reply.started":"2022-12-30T19:44:44.965753Z","shell.execute_reply":"2022-12-30T19:44:45.061254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_selection import SelectFromModel\nfrom sklearn.linear_model import LogisticRegression\n\nembeded_lr_selector = SelectFromModel(LogisticRegression(penalty=\"l2\", solver='lbfgs'), max_features=num_feats)\nembeded_lr_selector.fit(X_norm, y)\n\nembeded_lr_support = embeded_lr_selector.get_support()\nembeded_lr_feature = X.loc[:,embeded_lr_support].columns.tolist()\nprint(str(len(embeded_lr_feature)), 'selected features')","metadata":{"execution":{"iopub.status.busy":"2022-12-30T19:44:45.071341Z","iopub.execute_input":"2022-12-30T19:44:45.071903Z","iopub.status.idle":"2022-12-30T19:44:45.137194Z","shell.execute_reply.started":"2022-12-30T19:44:45.071842Z","shell.execute_reply":"2022-12-30T19:44:45.135898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_selection import SelectFromModel\nfrom sklearn.ensemble import RandomForestClassifier\n\nembeded_rf_selector = SelectFromModel(RandomForestClassifier(n_estimators=100, criterion='entropy'), max_features=num_feats)\nembeded_rf_selector.fit(X, y)\n\nembeded_rf_support = embeded_rf_selector.get_support()\nembeded_rf_feature = X.loc[:,embeded_rf_support].columns.tolist()\nprint(str(len(embeded_rf_feature)), 'selected features')","metadata":{"execution":{"iopub.status.busy":"2022-12-30T19:44:45.139332Z","iopub.execute_input":"2022-12-30T19:44:45.140093Z","iopub.status.idle":"2022-12-30T19:44:45.499132Z","shell.execute_reply.started":"2022-12-30T19:44:45.140021Z","shell.execute_reply":"2022-12-30T19:44:45.498164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_selection import SelectFromModel\nfrom lightgbm import LGBMClassifier\n\nlgbc=LGBMClassifier(n_estimators=500, learning_rate=0.05, num_leaves=32, colsample_bytree=0.2,\n            reg_alpha=3, reg_lambda=1, min_split_gain=0.01, min_child_weight=40)\n\nembeded_lgb_selector = SelectFromModel(lgbc, max_features=num_feats)\nembeded_lgb_selector.fit(X, y)\n\nembeded_lgb_support = embeded_lgb_selector.get_support()\nembeded_lgb_feature = X.loc[:,embeded_lgb_support].columns.tolist()\nprint(str(len(embeded_lgb_feature)), 'selected features')","metadata":{"execution":{"iopub.status.busy":"2022-12-30T19:44:45.500630Z","iopub.execute_input":"2022-12-30T19:44:45.500901Z","iopub.status.idle":"2022-12-30T19:44:45.591753Z","shell.execute_reply.started":"2022-12-30T19:44:45.500870Z","shell.execute_reply":"2022-12-30T19:44:45.590804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# put all selection together\nfeature_name = X.columns\nfeature_selection_df = pd.DataFrame({'Feature':feature_name, 'Pearson':cor_support, 'Chi-2':chi_support, 'RFE':rfe_support, 'Logistics':embeded_lr_support,\n                                    'Random Forest':embeded_rf_support, 'LightGBM':embeded_lgb_support})\n# count the selected times for each feature\nfeature_selection_df['Total'] = np.sum(feature_selection_df, axis=1)\n# display the top 100\nfeature_selection_df = feature_selection_df.sort_values(['Total','Feature'] , ascending=False)\nfeature_selection_df.index = range(1, len(feature_selection_df)+1)\nfeature_selection_df.head(num_feats)","metadata":{"execution":{"iopub.status.busy":"2022-12-30T19:44:45.593763Z","iopub.execute_input":"2022-12-30T19:44:45.594169Z","iopub.status.idle":"2022-12-30T19:44:45.625910Z","shell.execute_reply.started":"2022-12-30T19:44:45.594116Z","shell.execute_reply":"2022-12-30T19:44:45.624909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# segregating dataset into features i.e., X and target variables i.e., y\nX = dt.drop(['target','resting_blood_pressure'],axis=1)\ny = dt['target']","metadata":{"execution":{"iopub.status.busy":"2022-12-30T19:44:45.627593Z","iopub.execute_input":"2022-12-30T19:44:45.627986Z","iopub.status.idle":"2022-12-30T19:44:45.634655Z","shell.execute_reply.started":"2022-12-30T19:44:45.627942Z","shell.execute_reply":"2022-12-30T19:44:45.633649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2,shuffle=True, random_state=5)","metadata":{"execution":{"iopub.status.busy":"2022-12-30T19:44:45.636268Z","iopub.execute_input":"2022-12-30T19:44:45.636539Z","iopub.status.idle":"2022-12-30T19:44:45.648522Z","shell.execute_reply.started":"2022-12-30T19:44:45.636507Z","shell.execute_reply":"2022-12-30T19:44:45.647330Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nX_train[['age','cholesterol','max_heart_rate_achieved','st_depression']] = scaler.fit_transform(X_train[['age','cholesterol','max_heart_rate_achieved','st_depression']])\nX_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-30T19:44:45.650172Z","iopub.execute_input":"2022-12-30T19:44:45.650595Z","iopub.status.idle":"2022-12-30T19:44:45.679655Z","shell.execute_reply.started":"2022-12-30T19:44:45.650547Z","shell.execute_reply":"2022-12-30T19:44:45.678815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test[['age','cholesterol','max_heart_rate_achieved','st_depression']] = scaler.transform(X_test[['age','cholesterol','max_heart_rate_achieved','st_depression']])\nX_test.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-30T19:44:45.680947Z","iopub.execute_input":"2022-12-30T19:44:45.681247Z","iopub.status.idle":"2022-12-30T19:44:45.709165Z","shell.execute_reply.started":"2022-12-30T19:44:45.681215Z","shell.execute_reply":"2022-12-30T19:44:45.708012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models = GetBasedModel()\nnames,results = BasedLine2(X_train, y_train,models)","metadata":{"execution":{"iopub.status.busy":"2022-12-30T19:46:37.828145Z","iopub.execute_input":"2022-12-30T19:46:37.828506Z","iopub.status.idle":"2022-12-30T19:47:53.609015Z","shell.execute_reply.started":"2022-12-30T19:46:37.828472Z","shell.execute_reply":"2022-12-30T19:47:53.608105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**SOFT VOTING**","metadata":{}},{"cell_type":"code","source":"import xgboost as xgb\nclf1=RandomForestClassifier(criterion='entropy',n_estimators=100)\n\nclf2=DecisionTreeClassifier()\nclf3=xgb.XGBClassifier(n_estimators= 1000)\nclf4=ExtraTreesClassifier(n_estimators= 500)\n\nclf5=GradientBoostingClassifier(n_estimators=100,max_features='sqrt')\n\n\neclf1 = VotingClassifier(estimators=[('rfe', clf1), ('decc', clf2), ('xgb', clf3),('ET',clf4),('gb',clf5),], \n                         voting='soft', weights=[4,1,2,3,1])\neclf1.fit(X_train,y_train)\ny_pred_sv =eclf1.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-12-30T19:48:01.143710Z","iopub.execute_input":"2022-12-30T19:48:01.144252Z","iopub.status.idle":"2022-12-30T19:48:03.924329Z","shell.execute_reply.started":"2022-12-30T19:48:01.144207Z","shell.execute_reply":"2022-12-30T19:48:03.923280Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**MODEL EVALUATION**","metadata":{}},{"cell_type":"code","source":"CM=confusion_matrix(y_test,y_pred_sv)\nsns.heatmap(CM, annot=True)\n\nTN = CM[0][0]\nFN = CM[1][0]\nTP = CM[1][1]\nFP = CM[0][1]\nspecificity = TN/(TN+FP)\nloss_log = log_loss(y_test, y_pred_sv)\nacc= accuracy_score(y_test, y_pred_sv)\nroc=roc_auc_score(y_test, y_pred_sv)\nprec = precision_score(y_test, y_pred_sv)\nrec = recall_score(y_test, y_pred_sv)\nf1 = f1_score(y_test, y_pred_sv)\n\nmathew = matthews_corrcoef(y_test, y_pred_sv)\nmodel_results =pd.DataFrame([['Soft Voting',acc, prec,rec,specificity, f1,roc, loss_log,mathew]],\n               columns = ['Model', 'Accuracy','Precision', 'Sensitivity','Specificity', 'F1 Score','ROC','Log_Loss','mathew_corrcoef'])\n\nmodel_results","metadata":{"execution":{"iopub.status.busy":"2022-12-30T19:48:03.927123Z","iopub.execute_input":"2022-12-30T19:48:03.927500Z","iopub.status.idle":"2022-12-30T19:48:04.211065Z","shell.execute_reply.started":"2022-12-30T19:48:03.927456Z","shell.execute_reply":"2022-12-30T19:48:04.209962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf_ent = RandomForestClassifier(criterion='entropy',n_estimators=100)\nrf_ent.fit(X_train, y_train)\ny_pred_rfe = rf_ent.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-12-30T19:48:04.213173Z","iopub.execute_input":"2022-12-30T19:48:04.213541Z","iopub.status.idle":"2022-12-30T19:48:04.496304Z","shell.execute_reply.started":"2022-12-30T19:48:04.213496Z","shell.execute_reply":"2022-12-30T19:48:04.495209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mlp = MLPClassifier()\nmlp.fit(X_train,y_train)\ny_pred_mlp = mlp.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-12-30T19:48:04.497993Z","iopub.execute_input":"2022-12-30T19:48:04.498466Z","iopub.status.idle":"2022-12-30T19:48:06.527320Z","shell.execute_reply.started":"2022-12-30T19:48:04.498418Z","shell.execute_reply":"2022-12-30T19:48:06.525678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"knn = KNeighborsClassifier(9)\nknn.fit(X_train,y_train)\ny_pred_knn = knn.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-12-30T19:48:06.539030Z","iopub.execute_input":"2022-12-30T19:48:06.543656Z","iopub.status.idle":"2022-12-30T19:48:06.597919Z","shell.execute_reply.started":"2022-12-30T19:48:06.543548Z","shell.execute_reply":"2022-12-30T19:48:06.596544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"et_500 = ExtraTreesClassifier(n_estimators= 500)\net_500.fit(X_train,y_train)\ny_pred_et500 = et_500.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-12-30T19:48:06.606457Z","iopub.execute_input":"2022-12-30T19:48:06.610507Z","iopub.status.idle":"2022-12-30T19:48:07.573579Z","shell.execute_reply.started":"2022-12-30T19:48:06.610421Z","shell.execute_reply":"2022-12-30T19:48:07.572747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb = xgb.XGBClassifier(n_estimators= 100)\nxgb.fit(X_train,y_train)\ny_pred_xgb = xgb.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-12-30T19:48:07.575443Z","iopub.execute_input":"2022-12-30T19:48:07.575821Z","iopub.status.idle":"2022-12-30T19:48:07.664006Z","shell.execute_reply.started":"2022-12-30T19:48:07.575776Z","shell.execute_reply":"2022-12-30T19:48:07.663141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svc = SVC(kernel='linear',gamma='auto',probability=True)\nsvc.fit(X_train,y_train)\ny_pred_svc = svc.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-12-30T19:48:07.668444Z","iopub.execute_input":"2022-12-30T19:48:07.668757Z","iopub.status.idle":"2022-12-30T19:48:07.745448Z","shell.execute_reply.started":"2022-12-30T19:48:07.668723Z","shell.execute_reply":"2022-12-30T19:48:07.744460Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sgd = SGDClassifier(max_iter=1000, tol=1e-4)\nsgd.fit(X_train,y_train)\ny_pred_sgd = sgd.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-12-30T19:48:07.746482Z","iopub.execute_input":"2022-12-30T19:48:07.746725Z","iopub.status.idle":"2022-12-30T19:48:07.759387Z","shell.execute_reply.started":"2022-12-30T19:48:07.746697Z","shell.execute_reply":"2022-12-30T19:48:07.758399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ada = AdaBoostClassifier()\nada.fit(X_train,y_train)\ny_pred_ada = ada.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-12-30T19:48:07.760580Z","iopub.execute_input":"2022-12-30T19:48:07.760886Z","iopub.status.idle":"2022-12-30T19:48:07.887320Z","shell.execute_reply.started":"2022-12-30T19:48:07.760857Z","shell.execute_reply":"2022-12-30T19:48:07.886366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"decc = DecisionTreeClassifier()\ndecc.fit(X_train,y_train)\ny_pred_decc = decc.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-12-30T19:48:07.888439Z","iopub.execute_input":"2022-12-30T19:48:07.888860Z","iopub.status.idle":"2022-12-30T19:48:07.900924Z","shell.execute_reply.started":"2022-12-30T19:48:07.888826Z","shell.execute_reply":"2022-12-30T19:48:07.899993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gbm = GradientBoostingClassifier(n_estimators=100,max_features='sqrt')\ngbm.fit(X_train,y_train)\ny_pred_gbm = gbm.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-12-30T19:48:07.902702Z","iopub.execute_input":"2022-12-30T19:48:07.903093Z","iopub.status.idle":"2022-12-30T19:48:08.007457Z","shell.execute_reply.started":"2022-12-30T19:48:07.903026Z","shell.execute_reply":"2022-12-30T19:48:08.006547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import xgboost as xgboost\n# selecting list of top performing models to be used in stacked ensemble method\nmodels = [\n    RandomForestClassifier(criterion='entropy',n_estimators=100),\n    MLPClassifier(),\n    RandomForestClassifier(criterion='gini',n_estimators=100),\n    KNeighborsClassifier(9),\n    ExtraTreesClassifier(n_estimators= 500),\n    ExtraTreesClassifier(n_estimators= 100),\n    xgboost.XGBClassifier(n_estimators= 1000),\n    xgboost.XGBClassifier(n_estimators= 100),\n    xgboost.XGBClassifier(n_estimators= 500),\n    xgboost.XGBClassifier(n_estimators= 2000),\n    xgboost.XGBClassifier(),\n    SGDClassifier(max_iter=1000, tol=1e-4),\n    \n    SVC(kernel='linear',gamma='auto',probability=True),\n    AdaBoostClassifier(),\n    DecisionTreeClassifier(),\n    LinearDiscriminantAnalysis(),\n    GradientBoostingClassifier(n_estimators=100,max_features='sqrt'),\n    ExtraTreesClassifier(n_estimators= 1000),\n]","metadata":{"execution":{"iopub.status.busy":"2022-12-30T19:48:08.009002Z","iopub.execute_input":"2022-12-30T19:48:08.009386Z","iopub.status.idle":"2022-12-30T19:48:08.018243Z","shell.execute_reply.started":"2022-12-30T19:48:08.009340Z","shell.execute_reply":"2022-12-30T19:48:08.017459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# initializing generalizer model i.e., MLP classifier in our case\nmodel = MLPClassifier()\n    \nmodel = model.fit(S_train, y_train)\ny_pred = model.predict(S_test)\nprint('Final prediction score: [%.8f]' % accuracy_score(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2022-12-30T19:48:08.019742Z","iopub.execute_input":"2022-12-30T19:48:08.020276Z","iopub.status.idle":"2022-12-30T19:48:10.083769Z","shell.execute_reply.started":"2022-12-30T19:48:08.020227Z","shell.execute_reply":"2022-12-30T19:48:10.082401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CM=confusion_matrix(y_test,y_pred)\nsns.heatmap(CM, annot=True)\n\nTN = CM[0][0]\nFN = CM[1][0]\nTP = CM[1][1]\nFP = CM[0][1]\nspecificity = TN/(TN+FP)\nloss_log = log_loss(y_test, y_pred)\nacc= accuracy_score(y_test, y_pred)\nroc=roc_auc_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nmathew = matthews_corrcoef(y_test, y_pred)\nmodel_results =pd.DataFrame([['STacked Classifier2',acc, prec,rec,specificity, f1,roc, loss_log,mathew]],\n               columns = ['Model', 'Accuracy','Precision', 'Sensitivity','Specificity', 'F1 Score','ROC','Log_Loss','mathew_corrcoef'])\n\nmodel_results","metadata":{"execution":{"iopub.status.busy":"2022-12-30T19:48:10.086227Z","iopub.execute_input":"2022-12-30T19:48:10.087423Z","iopub.status.idle":"2022-12-30T19:48:10.420433Z","shell.execute_reply.started":"2022-12-30T19:48:10.087352Z","shell.execute_reply":"2022-12-30T19:48:10.419565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = {'Random Forest2': y_pred_rfe, \n                'MLP2': y_pred_mlp, \n                'KNN2': y_pred_knn, \n                'EXtra tree classifier2': y_pred_et500,\n                'XGB2': y_pred_xgb, \n                'SVC2': y_pred_svc, \n                'SGD2': y_pred_sgd,\n                'Adaboost2': y_pred_ada, \n                'CART2': y_pred_decc, \n                'GBM2': y_pred_gbm }\n\nmodels = pd.DataFrame(data) \n \nfor column in models:\n    CM=confusion_matrix(y_test,models[column])\n    \n    TN = CM[0][0]\n    FN = CM[1][0]\n    TP = CM[1][1]\n    FP = CM[0][1]\n    specificity = TN/(TN+FP)\n    loss_log = log_loss(y_test, models[column])\n    acc= accuracy_score(y_test, models[column])\n    roc=roc_auc_score(y_test, models[column])\n    prec = precision_score(y_test, models[column])\n    rec = recall_score(y_test, models[column])\n    f1 = f1_score(y_test, models[column])\n    \n    mathew = matthews_corrcoef(y_test, models[column])\n    results =pd.DataFrame([[column,acc, prec,rec,specificity, f1,roc, loss_log,mathew]],\n               columns = ['Model', 'Accuracy','Precision', 'Sensitivity','Specificity', 'F1 Score','ROC','Log_Loss','mathew_corrcoef'])\n    model_results = model_results.append(results, ignore_index = True)\n\nmodel_results","metadata":{"execution":{"iopub.status.busy":"2022-12-30T19:48:10.422197Z","iopub.execute_input":"2022-12-30T19:48:10.422469Z","iopub.status.idle":"2022-12-30T19:48:10.577870Z","shell.execute_reply.started":"2022-12-30T19:48:10.422438Z","shell.execute_reply":"2022-12-30T19:48:10.576993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**ROC AUC PLOT**","metadata":{}},{"cell_type":"code","source":"def roc_auc_plot(y_true, y_proba, label=' ', l='-', lw=1.0):\n    from sklearn.metrics import roc_curve, roc_auc_score\n    fpr, tpr, _ = roc_curve(y_true, y_proba[:,1])\n    ax.plot(fpr, tpr, linestyle=l, linewidth=lw,\n            label=\"%s (area=%.3f)\"%(label,roc_auc_score(y_true, y_proba[:,1])))\n\nf, ax = plt.subplots(figsize=(12,8))\n\nroc_auc_plot(y_test,model.predict_proba(S_test),label='Stacked Classifier ',l='-')\nroc_auc_plot(y_test,rf_ent.predict_proba(X_test),label='Random Forest Classifier ',l='-')\nroc_auc_plot(y_test,et_500.predict_proba(X_test),label='Extra Tree Classifier ',l='-')\nroc_auc_plot(y_test,xgb.predict_proba(X_test),label='XGboost',l='-')\n\nax.plot([0,1], [0,1], color='k', linewidth=0.5, linestyle='--', \n        )    \nax.legend(loc=\"lower right\")    \nax.set_xlabel('False Positive Rate')\nax.set_ylabel('True Positive Rate')\nax.set_xlim([0, 1])\nax.set_ylim([0, 1])\nax.set_title('Receiver Operator Characteristic curves')\nsns.despine()","metadata":{"execution":{"iopub.status.busy":"2022-12-30T19:48:10.579074Z","iopub.execute_input":"2022-12-30T19:48:10.579319Z","iopub.status.idle":"2022-12-30T19:48:10.979073Z","shell.execute_reply.started":"2022-12-30T19:48:10.579292Z","shell.execute_reply":"2022-12-30T19:48:10.977917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**PRECISION RECALL CURVES**","metadata":{}},{"cell_type":"code","source":"def precision_recall_plot(y_true, y_proba, label=' ', l='-', lw=1.0):\n    from sklearn.metrics import precision_recall_curve, average_precision_score\n    precision, recall, _ = precision_recall_curve(y_test,\n                                                  y_proba[:,1])\n    average_precision = average_precision_score(y_test, y_proba[:,1],\n                                                     average=\"micro\")\n    ax.plot(recall, precision, label='%s (average=%.3f)'%(label,average_precision),\n            linestyle=l, linewidth=lw)\n\nf, ax = plt.subplots(figsize=(14,10))\nprecision_recall_plot(y_test,model.predict_proba(S_test),label='Stacking classifier ',l='-')\nprecision_recall_plot(y_test,rf_ent.predict_proba(X_test),label='Random Forest Classifier ',l='-')\nprecision_recall_plot(y_test,et_500.predict_proba(X_test),label='Extra Tree Classifier ',l='-')\nprecision_recall_plot(y_test,xgb.predict_proba(X_test),label='XGboost',l='-')\nax.set_xlabel('Recall')\nax.set_ylabel('Precision')\nax.legend(loc=\"lower left\")\nax.grid(True)\nax.set_xlim([0, 1])\nax.set_ylim([0, 1])\nax.set_title('Precision-recall curves')\nsns.despine()","metadata":{"execution":{"iopub.status.busy":"2022-12-30T19:48:10.980775Z","iopub.execute_input":"2022-12-30T19:48:10.981154Z","iopub.status.idle":"2022-12-30T19:48:11.388193Z","shell.execute_reply.started":"2022-12-30T19:48:10.981108Z","shell.execute_reply":"2022-12-30T19:48:11.387213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**FEATURE IMPORTANCE**","metadata":{}},{"cell_type":"code","source":"feat_importances = pd.Series(rf_ent.feature_importances_, index=X_train.columns)\nfeat_importances.nlargest(20).plot(kind='barh')","metadata":{"execution":{"iopub.status.busy":"2022-12-30T19:48:11.389576Z","iopub.execute_input":"2022-12-30T19:48:11.389830Z","iopub.status.idle":"2022-12-30T19:48:11.631317Z","shell.execute_reply.started":"2022-12-30T19:48:11.389801Z","shell.execute_reply":"2022-12-30T19:48:11.630283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**CONCLUSION**","metadata":{}},{"cell_type":"markdown","source":"\n**The top 5 most contribution features are:**\n1. Max heart Rate achieved\n1. Cholestrol\n1. st_depression\n1. Age\n1. exercise_induced_angina","metadata":{}}]}